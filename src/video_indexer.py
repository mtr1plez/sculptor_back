# src/video_indexer.py
import os
import cv2
import json
import torch
import numpy as np
from PIL import Image
from tqdm import tqdm
from scenedetect import VideoManager, SceneManager
from scenedetect.detectors import ContentDetector
from sentence_transformers import SentenceTransformer

from utils import load_config

def detect_scenes(video_path, threshold=27.0, min_duration=1.0):
    """–®–∞–≥ 1: –ù–∞—Ä–µ–∑–∫–∞ –≤–∏–¥–µ–æ –Ω–∞ —Å—Ü–µ–Ω—ã"""
    print(f"‚úÇÔ∏è –ò—â–µ–º —Å—Ü–µ–Ω—ã –≤ {os.path.basename(video_path)}...")
    
    video_manager = VideoManager([video_path])
    scene_manager = SceneManager()
    scene_manager.add_detector(ContentDetector(threshold=threshold, min_scene_len=min_duration))
    
    video_manager.start()
    scene_manager.detect_scenes(frame_source=video_manager, show_progress=True)
    scene_list = scene_manager.get_scene_list()
    
    scenes = []
    scene_id = 0  # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–π ID
    
    for start, end in scene_list:
        duration = end.get_seconds() - start.get_seconds()
        if duration < min_duration:
            continue
            
        scenes.append({
            "id": scene_id,  # 0, 1, 2, 3, 4... –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ!
            "start_time": start.get_seconds(),
            "end_time": end.get_seconds(),
            "duration": duration,
            "frame_path": ""
        })
        scene_id += 1  # –ò–Ω–∫—Ä–µ–º–µ–Ω—Ç —Ç–æ–ª—å–∫–æ –¥–ª—è –≤–∞–ª–∏–¥–Ω—ã—Ö —Å—Ü–µ–Ω
    
    print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(scenes)} —Å—Ü–µ–Ω.")
    return scenes

def extract_frames(video_path, scenes, output_dir, image_size=224):
    """–®–∞–≥ 2: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–∞–¥—Ä–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–π —Å—Ü–µ–Ω—ã"""
    print("üì∏ –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–∞–¥—Ä—ã...")
    cap = cv2.VideoCapture(video_path)
    
    if not cap.isOpened():
        raise IOError(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–∫—Ä—ã—Ç—å –≤–∏–¥–µ–æ: {video_path}")

    valid_scenes = []

    for scene in tqdm(scenes):
        # –ë–µ—Ä–µ–º –∫–∞–¥—Ä –∏–∑ —Å–µ—Ä–µ–¥–∏–Ω—ã —Å—Ü–µ–Ω—ã
        mid_time = scene["start_time"] + (scene["duration"] / 2)
        
        # –ü–µ—Ä–µ–º–∞—Ç—ã–≤–∞–µ–º –≤–∏–¥–µ–æ –Ω–∞ –Ω—É–∂–Ω—ã–π –º–æ–º–µ–Ω—Ç (–≤ –º–∏–ª–ª–∏—Å–µ–∫—É–Ω–¥–∞—Ö)
        cap.set(cv2.CAP_PROP_POS_MSEC, mid_time * 1000)
        ret, frame = cap.read()
        
        if ret:
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º BGR (OpenCV) -> RGB (PIL)
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            img = Image.fromarray(frame_rgb)
            
            # –†–µ—Å–∞–π–∑ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –º–µ—Å—Ç–∞ –∏ —Å–∫–æ—Ä–æ—Å—Ç–∏
            img = img.resize((image_size, image_size))
            
            filename = f"scene_{scene['id']}.jpg"
            filepath = os.path.join(output_dir, filename)
            img.save(filepath, quality=80)
            
            scene["frame_path"] = filepath
            valid_scenes.append(scene)
        else:
            print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å –∫–∞–¥—Ä –¥–ª—è —Å—Ü–µ–Ω—ã {scene['id']}")

    cap.release()
    return valid_scenes

def embed_scenes(scenes, model_name, device):
    """–®–∞–≥ 3: –°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–æ–≤ —á–µ—Ä–µ–∑ CLIP"""
    print(f"üß† –ó–∞–≥—Ä—É–∂–∞–µ–º CLIP ({model_name}) –Ω–∞ {device}...")
    model = SentenceTransformer(model_name, device=device)
    
    image_paths = [s["frame_path"] for s in scenes]
    
    print("‚ö° –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ (–≤–µ–∫—Ç–æ—Ä—ã)...")
    # batch_size=32 –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ –¥–ª—è 3050Ti
    embeddings = model.encode(
        [Image.open(p) for p in image_paths], 
        batch_size=32, 
        convert_to_tensor=False, 
        show_progress_bar=True
    )
    
    return embeddings

def run_indexer():
    cfg = load_config()
    
    video_path = cfg["paths"]["input_video"]
    cache_dir = cfg["paths"]["cache_dir"]
    frames_dir = cfg["paths"]["frames_dir"]
    index_path = os.path.join(cache_dir, "scene_index.json")
    emb_path = os.path.join(cache_dir, "embeddings.npy")

    # 0. –ü—Ä–æ–≤–µ—Ä–∫–∞: –ï—Å–ª–∏ –∏–Ω–¥–µ–∫—Å —É–∂–µ –µ—Å—Ç—å, –Ω–µ –¥–µ–ª–∞–µ–º —Ä–∞–±–æ—Ç—É –¥–≤–∞–∂–¥—ã
    if os.path.exists(index_path) and os.path.exists(emb_path):
        print("üìÇ –ò–Ω–¥–µ–∫—Å —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é.")
        # –¢—É—Ç –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –ª–æ–≥–∏–∫—É "force update", –µ—Å–ª–∏ –Ω–∞–¥–æ
        return

    # 1. –î–µ—Ç–µ–∫—Ü–∏—è
    scenes = detect_scenes(
        video_path, 
        threshold=cfg["params"]["scene_threshold"],
        min_duration=cfg["params"]["min_scene_duration"]
    )
    
    # 2. –≠–∫—Å—Ç—Ä–∞–∫—Ü–∏—è –∫–∞–¥—Ä–æ–≤
    scenes = extract_frames(
        video_path, 
        scenes, 
        frames_dir, 
        image_size=cfg["params"]["image_size"]
    )
    
    # 3. –≠–º–±–µ–¥–¥–∏–Ω–≥
    embeddings = embed_scenes(
        scenes, 
        cfg["models"]["clip_model"], 
        cfg["models"]["device"]
    )
    
    # 4. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    print("üíæ –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ...")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º JSON (–º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ)
    with open(index_path, "w", encoding="utf-8") as f:
        json.dump(scenes, f, indent=4)
        
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º NPY (–≤–µ–∫—Ç–æ—Ä—ã)
    np.save(emb_path, embeddings)
    
    print("üéâ –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")

if __name__ == "__main__":
    # –î–ª—è —Ç–µ—Å—Ç–∞ –∑–∞–ø—É—Å–∫–∞–µ–º —Ñ—É–Ω–∫—Ü–∏—é –Ω–∞–ø—Ä—è–º—É—é
    run_indexer()