import os
import json
from pathlib import Path
from typing import List, Dict, Optional, Set
import numpy as np
from sentence_transformers import SentenceTransformer
import re
import random

from utils import load_config


class SmartMatcher:
    """–£–º–Ω—ã–π –º–∞—Ç—á–µ—Ä —Å —É—á–µ—Ç–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π"""
    
    def __init__(self, config_path: str = "config.yaml"):
        self.cfg = load_config(config_path)
        self.device = self.cfg["models"]["device"]
        self.clip_model = self.cfg["models"]["clip_model"]
        
        print(f"üß† –ó–∞–≥—Ä—É–∂–∞—é {self.clip_model} –Ω–∞ {self.device}...")
        self.model = SentenceTransformer(self.clip_model, device=self.device)
        
        # === –ö–û–ù–¢–ï–ö–°–¢ –ò –ü–ï–†–°–û–ù–ê–ñ–ò ===
        self.active_character = None
        
        # === –†–û–¢–ê–¶–ò–Ø –ò COOLDOWN ===
        self.frame_usage_count = {}
        self.frame_last_used_at = {}
        self.max_frame_usage = 3
        self.min_frame_cooldown = 20  # –ë–∞–∑–æ–≤—ã–π cooldown
        self.top_candidates_pool = 5
        
        # === CONTINUITY (–ù–ï–ü–†–ï–†–´–í–ù–û–°–¢–¨) ===
        self.continuity_bonus = 0.05
        self.scene_continuity_window = 48  # ~2 —Å–µ–∫—É–Ω–¥—ã –ø—Ä–∏ 24fps
        self.last_selected_frame_idx = None
        
        # === –°–õ–û–í–ê–†–¨ –ò–ú–ï–ù (–∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏) ===
        self.name_translations = {}
    
    def load_character_names(self, names_dict_path: str) -> Dict[str, Dict]:
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ —Å–ª–æ–≤–∞—Ä—è –≤–∞—Ä–∏–∞—Ü–∏–π –∏–º–µ–Ω –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π
        
        Args:
            names_dict_path: –ü—É—Ç—å –∫ character_names.json
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å –≤–∞—Ä–∏–∞—Ü–∏–π
        """
        try:
            with open(names_dict_path, "r", encoding="utf-8") as f:
                return json.load(f)
        except FileNotFoundError:
            print(f"‚ö†Ô∏è –°–ª–æ–≤–∞—Ä—å –∏–º–µ–Ω –Ω–µ –Ω–∞–π–¥–µ–Ω: {names_dict_path}")
            print("   –ó–∞–ø—É—Å—Ç–∏ character_detector.py –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Å–ª–æ–≤–∞—Ä—è")
            return {}
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å–ª–æ–≤–∞—Ä—è: {e}")
            return {}
    
    def build_translation_map(self, char_names_dict: Dict[str, Dict]) -> Dict[str, str]:
        """
        –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –ø–ª–æ—Å–∫–æ–≥–æ —Å–ª–æ–≤–∞—Ä—è –ø–µ—Ä–µ–≤–æ–¥–æ–≤ –∏–∑ —Å–ª–æ–≤–∞—Ä—è –≤–∞—Ä–∏–∞—Ü–∏–π
        
        Args:
            char_names_dict: –°–ª–æ–≤–∞—Ä—å –≤–∞—Ä–∏–∞—Ü–∏–π –æ—Ç character_detector
        
        Returns:
            –ü–ª–æ—Å–∫–∏–π —Å–ª–æ–≤–∞—Ä—å {–≤–∞—Ä–∏–∞—Ü–∏—è: –æ—Å–Ω–æ–≤–Ω–æ–µ_–∏–º—è}
        """
        translations = {}
        
        for main_name, variations in char_names_dict.items():
            # –í—Å–µ —Ä—É—Å—Å–∫–∏–µ –≤–∞—Ä–∏–∞—Ü–∏–∏
            for rus_var in variations.get('russian', []):
                translations[rus_var.lower()] = main_name
            
            # –í—Å–µ –∞–Ω–≥–ª–∏–π—Å–∫–∏–µ –≤–∞—Ä–∏–∞—Ü–∏–∏
            for eng_var in variations.get('english', []):
                translations[eng_var.lower()] = main_name
            
            # –í—Å–µ –∞–ª–∏–∞—Å—ã
            for alias in variations.get('aliases', []):
                translations[alias.lower()] = main_name
        
        return translations
    
    def load_transcript(self, transcript_path: str) -> List[Dict]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–∞"""
        with open(transcript_path, "r", encoding="utf-8") as f:
            return json.load(f)
    
    def load_character_map(self, character_map_path: str) -> Dict[str, List[int]]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∫–∞—Ä—Ç—ã –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π"""
        with open(character_map_path, "r", encoding="utf-8") as f:
            return json.load(f)
    
    def extract_character_names(self, text: str, known_characters: List[str]) -> List[str]:
        """
        –£–º–Ω—ã–π NER —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞
        
        Args:
            text: –¢–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ (–º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–∞ —Ä—É—Å—Å–∫–æ–º)
            known_characters: –°–ø–∏—Å–æ–∫ –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π (–Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º)
        
        Returns:
            –°–ø–∏—Å–æ–∫ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π (–Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º)
        """
        found = []
        text_lower = text.lower()
        
        # –°–Ω–∞—á–∞–ª–∞ –∏—â–µ–º —Ä—É—Å—Å–∫–∏–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –∏–º–µ–Ω
        for rus_name, eng_name in self.name_translations.items():
            pattern = r'\b' + re.escape(rus_name) + r'\b'
            if re.search(pattern, text_lower):
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —Ç–∞–∫–æ–π –ø–µ—Ä—Å–æ–Ω–∞–∂ –µ—Å—Ç—å –≤ known_characters
                if eng_name in known_characters and eng_name not in found:
                    found.append(eng_name)
        
        # –ü–æ—Ç–æ–º –ø—Ä–æ–≤–µ—Ä—è–µ–º –∞–Ω–≥–ª–∏–π—Å–∫–∏–µ –∏–º–µ–Ω–∞ –Ω–∞–ø—Ä—è–º—É—é (–µ—Å–ª–∏ –µ—Å—Ç—å)
        for char in known_characters:
            pattern = r'\b' + re.escape(char.lower()) + r'\b'
            if re.search(pattern, text_lower) and char not in found:
                found.append(char)
        
        return found
    
    def get_search_pool(
        self, 
        text: str, 
        character_map: Dict[str, List[int]],
        all_scene_ids: List[int]
    ) -> tuple[List[int], str]:
        """
        –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—É–ª–∞ —Å—Ü–µ–Ω —Å —É–º–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
        
        Args:
            text: –¢–µ–∫—Å—Ç —á–∞–Ω–∫–∞
            character_map: –ö–∞—Ä—Ç–∞ –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π
            all_scene_ids: –í—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ ID —Å—Ü–µ–Ω
        
        Returns:
            (–ø—É–ª —Å—Ü–µ–Ω, –æ–ø–∏—Å–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞)
        """
        known_characters = [char for char in character_map.keys() if char != "none"]
        
        # 1. –ò—â–µ–º —è–≤–Ω—ã–µ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –∏–º–µ–Ω
        mentioned_characters = self.extract_character_names(text, known_characters)
        
        if mentioned_characters:
            # –ù–∞—à–ª–∏ –∫–æ–≥–æ-—Ç–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ
            primary_char = mentioned_characters[0]
            
            # –ï—Å–ª–∏ —ç—Ç–æ –ù–û–í–´–ô –ø–µ—Ä—Å–æ–Ω–∞–∂, –ø–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è
            if primary_char != self.active_character:
                self.active_character = primary_char
                context = f"—Å–º–µ–Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ -> {self.active_character}"
            else:
                context = f"–ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ -> {self.active_character}"
        
        # 2. –ï—Å–ª–∏ –∏–º–µ–Ω –Ω–µ—Ç, –ø—Ä–æ–≤–µ—Ä—è–µ–º "–ª–∏–ø–∫–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç"
        elif self.active_character:
            # –û—Å—Ç–∞–µ–º—Å—è –Ω–∞ —Å—Ç–∞—Ä–æ–º –ø–µ—Ä—Å–æ–Ω–∞–∂–µ
            context = f"—É–¥–µ—Ä–∂–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ ({self.active_character})"
        else:
            context = "–±–µ–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ (–æ–±—â–∏–π –ø–æ–∏—Å–∫)"
        
        # 3. –í—ã–±–∏—Ä–∞–µ–º –ø—É–ª —Å—Ü–µ–Ω
        if self.active_character:
            pool = character_map.get(self.active_character, [])
            # –ï—Å–ª–∏ —É –ø–µ—Ä—Å–æ–Ω–∞–∂–∞ –Ω–µ—Ç –∫–∞–¥—Ä–æ–≤, fallback –Ω–∞ –≤—Å–µ—Ö
            if not pool:
                pool = all_scene_ids
                context += " (–ø—É–ª –ø—É—Å—Ç -> fallback)"
        else:
            pool = all_scene_ids
        
        return pool, context
    
    def find_best_match_with_rotation(
        self,
        text_emb: np.ndarray,
        pool_embeddings: np.ndarray,
        pool_indices: List[int],
        frame_files: List[Path],
        current_chunk_idx: int
    ) -> tuple[int, float]:
        """
        –ü–æ–∏—Å–∫ –ª—É—á—à–µ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è —Å —Ä–æ—Ç–∞—Ü–∏–µ–π, cooldown, —Ä–∞–Ω–¥–æ–º–∏–∑–∞—Ü–∏–µ–π –∏ continuity
        
        Args:
            text_emb: –≠–º–±–µ–¥–¥–∏–Ω–≥ —Ç–µ–∫—Å—Ç–∞
            pool_embeddings: –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ –∫–∞–¥—Ä–æ–≤ –∏–∑ –ø—É–ª–∞
            pool_indices: –ò–Ω–¥–µ–∫—Å—ã –∫–∞–¥—Ä–æ–≤ –≤ –ø—É–ª–µ
            frame_files: –°–ø–∏—Å–æ–∫ –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤ –∫–∞–¥—Ä–æ–≤
            current_chunk_idx: –¢–µ–∫—É—â–∏–π –∏–Ω–¥–µ–∫—Å —á–∞–Ω–∫–∞
        
        Returns:
            (–∏–Ω–¥–µ–∫—Å –ª—É—á—à–µ–≥–æ –∫–∞–¥—Ä–∞, score)
        """
        # 1. –ë–∞–∑–æ–≤—ã–π Semantic Similarity (–∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ)
        base_scores = pool_embeddings @ text_emb
        
        # 2. –î–æ–±–∞–≤–ª—è–µ–º –±–æ–Ω—É—Å –∑–∞ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ—Å—Ç—å (Continuity Bonus)
        # –ï—Å–ª–∏ –∫–∞–¥—Ä –Ω–∞—Ö–æ–¥–∏—Ç—Å—è —Ä—è–¥–æ–º —Å –ø—Ä–µ–¥—ã–¥—É—â–∏–º, –æ–Ω –ø–æ–ª—É—á–∞–µ—Ç –±—É—Å—Ç
        final_scores = base_scores.copy()
        
        if self.last_selected_frame_idx is not None:
            for i, real_idx in enumerate(pool_indices):
                dist = abs(real_idx - self.last_selected_frame_idx)
                
                # –ë–æ–Ω—É—Å —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –∫–∞–¥—Ä –±–ª–∏–∑–∫–æ, –Ω–æ –Ω–µ —Ç–æ—Ç –∂–µ —Å–∞–º—ã–π
                # –í–ê–ñ–ù–û: dist –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –±–æ–ª—å—à–∏–º, —á—Ç–æ–±—ã –Ω–µ –±—Ä–∞—Ç—å —Å–æ—Å–µ–¥–Ω–∏–π –∫–∞–¥—Ä
                if 5 < dist < self.scene_continuity_window:
                    final_scores[i] += self.continuity_bonus
        
        # 3. –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π cooldown –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞–∑–º–µ—Ä–∞ –ø—É–ª–∞
        pool_size = len(pool_indices)
        dynamic_cooldown = max(5, min(20, pool_size // 10))
        # 20 –∫–∞–¥—Ä–æ–≤ ‚Üí cooldown 5
        # 100 –∫–∞–¥—Ä–æ–≤ ‚Üí cooldown 10  
        # 300+ –∫–∞–¥—Ä–æ–≤ ‚Üí cooldown 20
        
        # 4. –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤
        sorted_indices = np.argsort(final_scores)[::-1]
        candidates = []
        
        for pool_idx in sorted_indices:
            real_idx = pool_indices[pool_idx]
            frame_name = frame_files[real_idx].name
            
            usage_count = self.frame_usage_count.get(frame_name, 0)
            last_used = self.frame_last_used_at.get(frame_name, -999)
            
            # –õ–∏–º–∏—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–π
            if usage_count >= self.max_frame_usage:
                continue
            
            # –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π cooldown
            if (current_chunk_idx - last_used) < dynamic_cooldown:
                continue
            
            candidates.append({
                'real_idx': real_idx,
                'score': float(final_scores[pool_idx]),
                'frame_name': frame_name
            })
            
            # –°–æ–±–∏—Ä–∞–µ–º —Ç–æ–ø-N –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤
            if len(candidates) >= self.top_candidates_pool:
                break
        
        # 5. –í—ã–±–æ—Ä –ø–æ–±–µ–¥–∏—Ç–µ–ª—è
        if candidates:
            # Weighted random choice - –ª—É—á—à–∏–µ –∫–∞–¥—Ä—ã —á–∞—â–µ –≤—ã–±–∏—Ä–∞—é—Ç—Å—è
            weights = [c['score'] for c in candidates]
            chosen = random.choices(candidates, weights=weights)[0]
        else:
            # Fallback: –µ—Å–ª–∏ –≤—Å–µ –≤ cooldown, –±–µ—Ä–µ–º –ª—É—á—à–∏–π –ø–æ score
            best_idx_raw = np.argmax(final_scores)
            chosen = {
                'real_idx': pool_indices[best_idx_raw],
                'score': float(final_scores[best_idx_raw]),
                'frame_name': frame_files[pool_indices[best_idx_raw]].name
            }
        
        # 6. –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è
        self.frame_usage_count[chosen['frame_name']] = \
            self.frame_usage_count.get(chosen['frame_name'], 0) + 1
        self.frame_last_used_at[chosen['frame_name']] = current_chunk_idx
        self.last_selected_frame_idx = chosen['real_idx']
        
        return chosen['real_idx'], chosen['score']
    
    def normalize_vector(self, vec: np.ndarray) -> np.ndarray:
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ–∫—Ç–æ—Ä–∞"""
        norm = np.linalg.norm(vec)
        return vec / norm if norm > 0 else vec
    
    def match_audio_to_frames(
        self,
        transcript_path: str = None,
        character_map_path: str = None,
        names_dict_path: str = None,
        frames_dir: str = None,
        embeddings_path: str = None,
        output_path: str = None
    ) -> List[Dict]:
        """
        –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —É–º–Ω–æ–≥–æ –º–∞—Ç—á–∏–Ω–≥–∞
        
        Args:
            transcript_path: –ü—É—Ç—å –∫ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç—É
            character_map_path: –ü—É—Ç—å –∫ –∫–∞—Ä—Ç–µ –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π
            names_dict_path: –ü—É—Ç—å –∫ —Å–ª–æ–≤–∞—Ä—é –≤–∞—Ä–∏–∞—Ü–∏–π –∏–º–µ–Ω
            frames_dir: –ü—É—Ç—å –∫ –ø–∞–ø–∫–µ —Å —Ñ—Ä–µ–π–º–∞–º–∏
            embeddings_path: –ü—É—Ç—å –∫ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º —Ñ—Ä–µ–π–º–æ–≤
            output_path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        
        Returns:
            –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –º–∞—Ç—á–∏–Ω–≥–∞
        """
        # –î–µ—Ñ–æ–ª—Ç–Ω—ã–µ –ø—É—Ç–∏
        if transcript_path is None:
            transcript_path = os.path.join(
                self.cfg["paths"]["cache_dir"],
                "transcript.json"
            )
        if character_map_path is None:
            character_map_path = os.path.join(
                self.cfg["paths"]["cache_dir"],
                "character_frames.json"
            )
        if names_dict_path is None:
            names_dict_path = os.path.join(
                self.cfg["paths"]["cache_dir"],
                "character_names.json"
            )
        if frames_dir is None:
            frames_dir = self.cfg["paths"]["frames_dir"]
        if embeddings_path is None:
            embeddings_path = os.path.join(
                self.cfg["paths"]["cache_dir"],
                "embeddings.npy"
            )
        if output_path is None:
            output_path = os.path.join(
                self.cfg["paths"]["cache_dir"],
                "edit_plan.json"
            )
        
        print("üìÑ –ó–∞–≥—Ä—É–∂–∞—é –¥–∞–Ω–Ω—ã–µ...")
        transcript = self.load_transcript(transcript_path)
        character_map = self.load_character_map(character_map_path)
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ —Å–ª–æ–≤–∞—Ä—è –∏–º–µ–Ω
        char_names_dict = self.load_character_names(names_dict_path)
        if char_names_dict:
            self.name_translations = self.build_translation_map(char_names_dict)
            print(f"üìö –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.name_translations)} –≤–∞—Ä–∏–∞—Ü–∏–π –∏–º–µ–Ω")
        else:
            print("‚ö†Ô∏è –†–∞–±–æ—Ç–∞—é –±–µ–∑ —Å–ª–æ–≤–∞—Ä—è –∏–º–µ–Ω (–º–æ–≥—É—Ç –±—ã—Ç—å –ø—Ä–æ–±–ª–µ–º—ã —Å —Ä—É—Å—Å–∫–∏–º)")
        
        print(f"‚úÖ –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç: {len(transcript)} —Å–µ–≥–º–µ–Ω—Ç–æ–≤")
        print(f"‚úÖ –ü–µ—Ä—Å–æ–Ω–∞–∂–∏: {', '.join([k for k in character_map.keys() if k != 'none'])}")
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏ —Ñ—Ä–µ–π–º–æ–≤
        print("\nüìÇ –ó–∞–≥—Ä—É–∂–∞—é —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —Ñ—Ä–µ–π–º–æ–≤...")
        frame_embeddings = np.load(embeddings_path)
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        if not np.allclose(np.linalg.norm(frame_embeddings[0]), 1.0):
            print("üîß –ù–æ—Ä–º–∞–ª–∏–∑—É—é —ç–º–±–µ–¥–¥–∏–Ω–≥–∏...")
            frame_embeddings = np.array([
                self.normalize_vector(emb) for emb in frame_embeddings
            ])
        
        # –°–ø–∏—Å–æ–∫ —Ñ—Ä–µ–π–º–æ–≤
        frames_path = Path(frames_dir)
        frame_files = sorted([
            f for f in frames_path.iterdir() 
            if f.suffix.lower() in {'.jpg', '.jpeg', '.png'}
        ])
        
        # –°–æ–∑–¥–∞–Ω–∏–µ —Å–ª–æ–≤–∞—Ä—è scene_id -> index
        scene_id_to_idx = {}
        for idx, frame_path in enumerate(frame_files):
            scene_id = int(frame_path.stem.split('_')[1])
            scene_id_to_idx[scene_id] = idx
        
        all_scene_ids = list(scene_id_to_idx.keys())
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è character_map - —É–±–∏—Ä–∞–µ–º —Å—Ü–µ–Ω—ã –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç –≤ frames
        print("üîç –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–∞—Ä—Ç—ã –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π...")
        invalid_scenes = 0
        for char, scenes in character_map.items():
            valid_scenes = [s for s in scenes if s in scene_id_to_idx]
            invalid_count = len(scenes) - len(valid_scenes)
            if invalid_count > 0:
                invalid_scenes += invalid_count
                character_map[char] = valid_scenes
        
        if invalid_scenes > 0:
            print(f"‚ö†Ô∏è –£–¥–∞–ª–µ–Ω–æ {invalid_scenes} –Ω–µ—Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Å—Ü–µ–Ω –∏–∑ –∫–∞—Ä—Ç—ã –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π")
        
        print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(frame_files)} —Ñ—Ä–µ–π–º–æ–≤\n")
        
        # –ü–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Ç–µ–∫—Å—Ç–æ–≤
        texts = [segment["text"] for segment in transcript]
        print("‚ö° –ì–µ–Ω–µ—Ä–∏—Ä—É—é —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —Ç–µ–∫—Å—Ç–æ–≤...")
        text_embeddings = self.model.encode(
            texts,
            batch_size=32,
            convert_to_tensor=False,
            show_progress_bar=True,
            normalize_embeddings=True
        )
        
        # –£–º–Ω—ã–π –º–∞—Ç—á–∏–Ω–≥
        print("\nüéØ –£–º–Ω—ã–π –º–∞—Ç—á–∏–Ω–≥ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π...")
        results = []
        
        for idx, (text, text_emb) in enumerate(zip(texts, text_embeddings), 1):
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—É–ª –¥–ª—è –ø–æ–∏—Å–∫–∞
            search_pool, context = self.get_search_pool(
                text, 
                character_map, 
                all_scene_ids
            )
            
            # –§–∏–ª—å—Ç—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –ø–æ –ø—É–ª—É
            pool_indices = [scene_id_to_idx[sid] for sid in search_pool if sid in scene_id_to_idx]
            
            if not pool_indices:
                # Fallback - –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å–µ
                pool_indices = list(range(len(frame_embeddings)))
            
            pool_embeddings = frame_embeddings[pool_indices]
            
            # –ü–æ–∏—Å–∫ –ª—É—á—à–µ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è —Å —Ä–æ—Ç–∞—Ü–∏–µ–π –∏ cooldown
            best_idx, best_score = self.find_best_match_with_rotation(
                text_emb,
                pool_embeddings,
                pool_indices,
                frame_files,
                idx - 1  # –¢–µ–∫—É—â–∏–π –∏–Ω–¥–µ–∫—Å —á–∞–Ω–∫–∞ (0-based)
            )
            
            results.append({
                "audio_text": text,
                "frame_file": str(frame_files[best_idx]),
                "frame_index": best_idx,
                "similarity_score": best_score,
                "chunk_index": idx - 1,
                "search_context": context,
                "search_pool_size": len(pool_indices)
            })
            
            print(f"  [{idx}/{len(texts)}] '{text[:40]}...'")
            print(f"      ‚Üí {frame_files[best_idx].name} "
                  f"(score: {best_score:.3f})")
            print(f"      ‚Üí –ö–æ–Ω—Ç–µ–∫—Å—Ç: {context} | –ü—É–ª: {len(pool_indices)} —Å—Ü–µ–Ω")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        output_file = Path(output_path)
        output_file.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        print(f"\nüíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {output_path}")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∫–∞–¥—Ä–æ–≤
        print("\nüìä –¢–æ–ø-10 —Å–∞–º—ã—Ö –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –∫–∞–¥—Ä–æ–≤:")
        sorted_usage = sorted(
            self.frame_usage_count.items(), 
            key=lambda x: -x[1]
        )[:10]
        for frame_name, count in sorted_usage:
            print(f"   {frame_name}: {count} —Ä–∞–∑")
        
        print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞:")
        context_stats = {}
        for result in results:
            ctx = result["search_context"]
            context_stats[ctx] = context_stats.get(ctx, 0) + 1
        
        for ctx, count in sorted(context_stats.items(), key=lambda x: -x[1]):
            print(f"   {ctx}: {count} —á–∞–Ω–∫–æ–≤")
        
        print(f"\n‚úÖ –ì–æ—Ç–æ–≤–æ! –°–º–∞—Ç—á–µ–Ω–æ {len(results)} —á–∞–Ω–∫–æ–≤")
        return results


def main():
    """–û—Å–Ω–æ–≤–Ω–æ–π –∑–∞–ø—É—Å–∫"""
    matcher = SmartMatcher()
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è character_frames.json
    character_map_path = os.path.join(
        matcher.cfg["paths"]["cache_dir"],
        "character_frames.json"
    )
    
    names_dict_path = os.path.join(
        matcher.cfg["paths"]["cache_dir"],
        "character_names.json"
    )
    
    if not Path(character_map_path).exists():
        print("‚ùå –§–∞–π–ª character_frames.json –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        print("   –°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏: python src/character_detector.py")
        return
    
    if not Path(names_dict_path).exists():
        print("‚ö†Ô∏è –§–∞–π–ª character_names.json –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        print("   –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∑–∞–ø—É—Å—Ç–∏—Ç—å: python src/character_detector.py")
        print("   –ü—Ä–æ–¥–æ–ª–∂–∞—é –±–µ–∑ —Å–ª–æ–≤–∞—Ä—è –∏–º–µ–Ω...\n")
    
    # –£–º–Ω—ã–π –º–∞—Ç—á–∏–Ω–≥
    results = matcher.match_audio_to_frames()
    
    print("\n‚úÖ –¢–µ–ø–µ—Ä—å –º–æ–∂–Ω–æ –∑–∞–ø—É—Å—Ç–∏—Ç—å renderer.py –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –≤–∏–¥–µ–æ!")


if __name__ == "__main__":
    main()